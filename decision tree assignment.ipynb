{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebApmQkQi1p0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans- A Decision Tree is a supervised learning algorithm used to classify data into categories.\n",
        "\n",
        "It works like a flowchart where each node represents a decision based on a feature, each branch shows the outcome of that decision, and each leaf node represents a final class label.\n",
        "\n",
        "How it works:\n",
        "\n",
        "1. The algorithm selects the feature that best splits the data (using Gini Index or Information Gain).\n",
        "\n",
        "2. It divides the data into branches based on that feature's values.\n",
        "\n",
        "3.\n",
        "\n",
        "This process repeats for each branch until all data is classified or a stopping condition is reached.\n",
        "\n",
        "4. For a new sample, the tree is followed from root to leaf to predict the class.\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans - In a Decision Tree, we need to decide which feature to split on at each step. To do that, the algorithm measures how \"pure\" or \"impure\" a node is - that means how mixed the classes are.\n",
        "\n",
        "Two common impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "It measures how often a randomly chosen element would be incorrectly classified if it was labeled according to the class distribution in the node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Giní=1-(pi)2\n",
        "\n",
        "where pi = probability of class i in that node.\n",
        "\n",
        "Range: 0 to 0.5 (for binary classes)\n",
        "\n",
        "o 0→ completely pure (only one class)\n",
        "\n",
        "* 0.5 maximum impurity (classes evenly mixed)\n",
        "\n",
        "2. Entropy\n",
        "\n",
        "Entropy measures the amount of uncertainty or randomness in the data.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Entropy=-(pilog2pi)\n",
        "\n",
        "Range: 0 to 1\n",
        "\n",
        "o 0→ pure node (one class only)\n",
        "\n",
        "o 1maximum impurity (equal mix of classes)\n",
        "\n",
        "How They Impact Splits:\n",
        "\n",
        "When building the tree, the algorithm tries to reduce impurity at each split.\n",
        "\n",
        "It calculates impurity before and after splitting a node.\n",
        "\n",
        "The feature that gives the maximum reduction in impurity (called Information Gain) is chosen for the split.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans- Difference between Pre-Pruning and Post-Pruning in Decision Trees\n",
        "\n",
        "Aspect:-\n",
        "\n",
        "meaning  \n",
        "\n",
        "when applied\n",
        "\n",
        "how it works\n",
        "\n",
        "goal\n",
        "\n",
        "pre-pruning:-\n",
        "\n",
        "stops the tree from growing too large during traning\n",
        "\n",
        "while building the tree\n",
        "\n",
        "uses conditions like maximum depth,minimum sample per split or minimum information gain\n",
        "\n",
        "prevent overfitting early\n",
        "\n",
        "post-pruning:-\n",
        "\n",
        "grows the full tree first then removes unnecessary branches.\n",
        "\n",
        "after the complete tree is built\n",
        "\n",
        "evaluates each branch on validaion data and prunes if it dosen't improve accuracy\n",
        "\n",
        "simplify an already complex model\n",
        "\n",
        "Practical Advantages:\n",
        "\n",
        "Pre-Pruning: Saves time and computation, as the tree doesn't grow unnecessarily deep.\n",
        "\n",
        "Post-Pruning: Gives better accuracy because it first learns all patterns, then removes only the unhelpful ones.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans- Information Gain in Decision Trees\n",
        "\n",
        "Information Gain (IG) is a measure used to decide which feature to split on when building a Decision Tree.\n",
        "\n",
        "It shows how much \"information\" or reduction in impurity (uncertainty) is achieved after splitting the data on a particular feature.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Information Gain-Entropy (Parent)-(ni/nixEntropy (Child_i)) where nin_ini is the number of samples in each child node.\n",
        "\n",
        "Why It's Important:\n",
        "\n",
        "It helps the algorithm choose the best feature that gives the most pure (least mixed) child nodes.\n",
        "\n",
        "A higher Information Gain means a better split, leading to more accurate and efficient classification.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans- Real-World Äpplications of Decision Trees\n",
        "\n",
        "1. Medical Diagnosis:\n",
        "\n",
        "Used to predict diseases or treatment outcomes based on patient data.\n",
        "\n",
        "2. Finance and Banking:\n",
        "\n",
        "Used for credit scoring, loan approval, and fraud detection.\n",
        "\n",
        "3. Marketing:\n",
        "\n",
        "Helps identify potential customers and predict buying behavior.\n",
        "\n",
        "4. Manufacturing: Used for quality control and fault detection in production lines.\n",
        "\n",
        "5. Education: Predicts student performance or dropout risk based on academic data.\n",
        "\n",
        "Main Advantages\n",
        "\n",
        "Easy to understand and interpret (no complex math needed).\n",
        "\n",
        "Handles both categorical and numerical data.\n",
        "\n",
        "No need for data scaling or normalization.\n",
        "\n",
        "Can capture non-linear relationships.\n",
        "\n",
        "Main Limitations\n",
        "\n",
        "Prone to overfitting (especially with deep trees).\n",
        "\n",
        "Small data changes can alter the structure (unstable).\n",
        "\n",
        "Less accurate compared to ensemble models like Random Forests or Gradient Boosted Trees.\n",
        "\n",
        "Dataset Info:\n",
        "\n",
        "Iris Dataset for classification tasks (sklearn. datasets. load iris() or provided CSV).\n",
        "\n",
        "Boston Housing Dataset for regression tasks\n",
        "\n",
        "(sklearn.datasets. load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4yRcN5KjCFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Create and train the Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Step 7: Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NAYxhcKrudT",
        "outputId": "9aadbc14-de90-460f-a996-a2a30580fcfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLcBZ8bLt9sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "ZIxAmSnJuVNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train a Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Step 5: Train a fully-grown Decision Tree (no depth limit)\n",
        "tree_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 6: Compare results\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print (\"Accuracy with fully-grown tree:\", accuracy_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj24c4fguWAl",
        "outputId": "87b635ed-d591-4a73-b92d-5516862e91a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoT15WBiwtBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "BeSLjMw_w-8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 2: Load the Boston Housing dataset\n",
        "# (Note: load_boston() is deprecated, so we use fetch_openml instead)\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Step 7: Display feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po2xmIULxFgx",
        "outputId": "99a19094-b740-4f1e-ca10-1f25981748a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.588026315789474\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gl73LVxvyHEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "tj3cWTtdyjKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Step 5: Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6, 10]\n",
        "}\n",
        "\n",
        "# Step 6: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Print best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Step 8: Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnj5vRB1yjx-",
        "outputId": "25956cee-b09c-42bd-97c2-74652a5f4776"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Best Cross-Validation Accuracy: 0.9428571428571428\n",
            "Test Set Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ksvwkBZCz8Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "ZXw5-i2p0Ga3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7g4d0Jy0Rid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Handle missing values:\n",
        "\n",
        "for numerical data - fil with mean/median.\n",
        "\n",
        "for categorical data - fill with most frequent or a new \"missing\" category.\n",
        "\n",
        "# 2. encode categorical features:\n",
        "\n",
        "use label or onehotencoder to convert text data into numbers.\n",
        "\n",
        "# 3. train decision tree model:\n",
        "\n",
        "split data into train adn test sets.\n",
        "\n",
        "train using decisiontreeclassifier () and fit it on the training data.\n",
        "\n",
        "# 4. tune hyperparameters:\n",
        "\n",
        "use gridsearchCV to find the best max_depth, min_sample_split, etc.\n",
        "\n",
        "choose the model with the highest cross-validation accuracy.\n",
        "\n",
        "# 5. evaluate performance:\n",
        "\n",
        "use matrics like accuracy , precision, recal, f1-score, and confusion matri on the test data."
      ],
      "metadata": {
        "id": "irbJXth-05kb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doNe0HM-292_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}